== Change Data Capture With Kafka From MySql to Neo4j

This demo will show you how you can create a Change Data Capture (CDC) architecture with Kafka, to migrate the data and its changes from MySql (or a different relational database supported from Debezium) to Neo4j. In this way, it's possible to work simultaneously with the two different databases, without worrying about keeping them synchronized. The stack for this demo is as follows:

. A MySql instance, that will be the start of our event stream;
. Our first Kafka Connect instance with the Debezium Plugin, that will read the binlog of MySql and stream the events that occur in the database;
. A Kafka instance, to manage the messages created from Debezium;
. Our second Kafka Connect instance with the Neo4j Plugin, that will stream the messages from Kafka to the Neo4j instance, running, for each specified topic, a certain query to manage the possible types of operation that can occur in the MySql database: _c_ (create), _r_ (read), _u_ (update) and _d_ (delete);
. A Neo4j instance, that will be the end point of our event stream.

We will also need, to run Kafka and use the Avro message format, the following instances:

. A Zookeeper instance
. A SchemaRegistry instance

To see the messages flowing through Kafka, we will use a Confluent Control Center instance, deployed in the same network of the stack.

Here the relational model in MySql we will use for this demo:

image::./img/erModel.png[Paper]

Here the data model we want to have in Neo4j after the ingestion:

image::./img/DataModel.png[Paper]

Here a simplified container diagram (based on https://c4model.com[C4 Model^]) to have a high level understanding of how the main components of our stack are interacting with each other. The other containers are not mandatory to have a working CDC system so they are not included in this diagram.

== Starting the project

To start the demo, run from the directory of the project the following command:

image::./img/Kafka Demo Container Diagram.drawio.png[Paper]

[source,shell]
----
docker compose up -d
----

The command will use the _docker-compose.yaml_ file inside the project to create the stack that we will use in the demo, pulling the necessary images and building each Docker container.


After starting the stack, the MySql instance will launch the script _init-db.sql_ (mounted through a volume inside MySql), that will create the user _debezium_, with all the privileges necessary to access the binlog, and will create and the database _demo_ with two tables, _customers_ and _orders_.

After around a minute, the stack will be completely up and running, we can check its status:

[source,shell]
----
docker ps
----

You can see that we will have eight different containers, one for each service we defined in our _docker compose_ file. After around a minute, the Kafka Connect services will have found their broker (the Kafka instance) and will be ready to receive in input the necessary configurations to:

. Define the stream from MySql to Kafka, defining the configuration for Debezium Plugin
. Define the stream from Kafka to Neo4j, defining the configuration for the Kafka Connect Neo4j Plugin.

Just before starting to configure our connectors, we need to create the constraints of our graph, to prevent nodes duplication during the ingestion. In our case, we have a uniqueness constraint for each label in our graph (Customer and Order). To create these constraints, we can run the file _createNeo4jConstraint.sh_ inside our project, that will run the _.cypher_ file created inside the folder _neo4jConfiguration_, here the snippet of the code:

[source, cypher]
----
CREATE CONSTRAINT IF NOT EXISTS for (c:Customer) require c.customerId IS UNIQUE;
CREATE CONSTRAINT IF NOT EXISTS for (o:Order) require o.orderId IS UNIQUE;
----

== Configuring the Debezium Plugin

Now let's take a look at the configuration for the Debezium connector for Mysql, called _debezium_ in our stack. This connector will read the MySql binlog file and create, for each CRUD operation, an event that will be streamed to Kafka. The configuration is a Json file, containing different fields that will define how Debezium will create its messages.

[source,json]
----
{
  "name": "debeziumConfig",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "my-sql",
    "database.port": "3306",
    "database.user": "debezium",
    "database.password": "password",
    "database.server.id": "42",
    "database.server.name": "mysql_connector",
    "table.whitelist": "demo.customers,demo.orders",
    "database.history.kafka.bootstrap.servers": "broker:9092",
    "database.history.kafka.topic": "dbhistory.demo",
    "decimal.handling.mode": "double",
    "include.schema.changes": "true",
    "transforms": "dropTopicPrefix",
    "transforms.dropTopicPrefix.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.dropTopicPrefix.regex": "mysql_connector.demo.(.*)",
    "transforms.dropTopicPrefix.replacement": "$1",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8081"
  }
}
----
Just to understand the core of the configuration, note that:

. You must include in the Debezium configuration the credentials related to the user that will be used from the connector, in this case the user _debezium_ with password _password_.

. The _key/value.converter_ fields are mandatory to define the format of the messages that will be streamed from Debezium. In this case we used the Avro format for the messages, including in the configuration the address of the SchemaRegistry service that will keep the schema to decode the messages.

. The _table.whitelist_ field is used to stream events only from the tables we desire, in this case the tables _customers_ and _orders_ inside the database demo.

. The _transform_ fields are used to change the format of the message or to define, like in our case, the topic where the messages will be subscribed.

In this demo, we will have two topics, called _customers_ and _orders_, created by keeping only the name of the MySql table from the topic created from debezium, that is:

----
database.server.name (mysql_connector) + table.whitelist (demo.customers,ecc...)
----

So, let's suppose that the message is coming from the table _demo.customers_, the transformation will be:

----
mysql_connector.demo.customers -> customers
----

This transformation is not mandatory, depends on what you prefer to have as topic name. There are different possible transformations, for more information look https://debezium.io/documentation/reference/stable/transformations/index.html[here^].

Running the script _configDebeziumConnector.sh_ will create a new connector inside the Debezium instance, that will start to stream messages from the MySql instance as soon as it starts. Let's take a look at the messages created from Debezium by accessing one of the topic created from the connector (_customers_, _orders_) from the Confluent Control Center instance, located at http://localhost:9021.

Here an example of message:

[source, json]
----
{
  "before": null,
  "after": {
    "mysql_connector.demo.customers.Value": {
      "customerNumber": 103,
      "customerName": "Atelier graphique",
      "contactLastName": "Schmitt",
      "contactFirstName": "Carine ",
      "phone": "40.32.2555",
      "addressLine1": "54, rue Royale",
      "addressLine2": null,
      "city": "Nantes",
      "state": null,
      "postalCode": {
        "string": "44000"
      },
      "country": "France",
      "salesRepEmployeeNumber": {
        "int": 1370
      },
      "creditLimit": {
        "double": 21000.0
      }
    }
  },
  "source": {
    "version": "1.9.3.Final",
    "connector": "mysql",
    "name": "mysql_connector",
    "ts_ms": 1656420989704,
    "snapshot": {
      "string": "true"
    },
    "db": "demo",
    "sequence": null,
    "table": {
      "string": "customers"
    },
    "server_id": 0,
    "gtid": null,
    "file": "binlog.000002",
    "pos": 32307,
    "row": 0,
    "thread": null,
    "query": null
  },
  "op": "r",
  "ts_ms": {
    "long": 1656420989707
  },
  "transaction": null
}
----

The information useful for the injection are, most of the time, defined inside the fields _before_, _after_ and _op_. This last field is really important because is what tells us the type of event that happened in the database (in this case, _op_ is equal to _r_, meaning that the operation is of type read (snapshot of the database)).
Because the operation is of type _r_, there is nothing inside the _before_ field (a row can't exist before its creation) and the data is present only in the _after_ field. To manage the different type of events, we now need to define the configuration for the Kafka Connect Neo4j Plugin.

== Configuring the Kafka Connect Neo4j Plugin

To stream the messages from Kafka to Neo4j, we will use the Kafka Connect instance that has installed the Neo4j Plugin, called _connect_ in our stack. Like the Debezium Plugin, we need to define a connector that will handle the messages coming from the different topics we have created.
Let's have a look at the configuration:

[source,json]
----
{
  "name": "Neo4jSinkConnector",
  "config": {
    "topics": "customers,orders",
    "kafka.bootstrap.servers": "broker:9092",
    "connector.class": "streams.kafka.connect.sink.Neo4jSinkConnector",
    "key.converter": "io.confluent.connect.avro.AvroConverter",
    "key.converter.schema.registry.url": "http://schema-registry:8081",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://schema-registry:8081",
    "errors.retry.timeout": "-1",
    "errors.retry.delay.max.ms": "1000",
    "errors.tolerance": "all",
    "errors.log.enable": true,
    "errors.deadletterqueue.topic.name": "test-error-topic",
    "errors.log.include.messages": true,
    "errors.deadletterqueue.topic.replication.factor": 1,
    "neo4j.server.uri": "bolt://neo4j:7687",
    "neo4j.authentication.basic.username": "neo4j",
    "neo4j.authentication.basic.password": "connect",
    "neo4j.encryption.enabled": false,
    "neo4j.topic.cypher.customers": "WITH event CALL {WITH event WITH event WHERE event.op IN [\"c\",\"u\",\"r\"] WITH event[\"after\"] AS evData MERGE (c:Customer{customerId:evData.customerNumber}) ON CREATE SET c.name = evData.customerName ON MATCH SET c.name = evData.customerName UNION WITH event WITH event WHERE event.op IN [\"d\"] WITH event[\"before\"] AS evData MATCH (c:Customer{customerId:evData.customerNumber}) WITH c OPTIONAL MATCH (c)-[:PLACED_ORDER]->(o) DETACH DELETE c,o }",
    "neo4j.topic.cypher.orders": "WITH event CALL {WITH event WITH event WHERE event.op IN [\"c\",\"u\",\"r\"] WITH event[\"after\"] AS evData MERGE (c:Customer{customerId:evData.customerNumber}) MERGE (o:Order{orderId:evData.orderNumber}) ON CREATE SET o.status = evData.status ON MATCH SET o.status = evData.status MERGE (c)-[:PLACED_ORDER]->(o) UNION WITH event WITH event WHERE event.op IN [\"d\"] WITH event[\"before\"] AS evData MATCH (o:Order{orderId:evData.orderNumber}) DETACH DELETE o }"
  }
}
----

This configuration and the Debezium's one share some fields (like the _key/value.converter_), but the main difference between them are the fields with prefix _neo4j_, especially the field _neo4j.topic.cypher.<topic_name>_. In this field we can define how the connector will manage the messages coming from each topic he's subscribed, giving us the freedom to define, for each topic, different queries.

Keep in mind that, the Cypher querying is one of the different sink ingestion strategies present in the Neo4j plugin, for further information take a look https://neo4j.com/labs/kafka/4.0/kafka-connect/#kafka-connect-sink-strategies[here^]. In this case is necessary because, otherwise, the connector doesn't know how the manage the messages coming from Debezium.

Because we are handling the sink ingestion using the Cypher strategy, we need to manage, for each topic, each type of operation (CRUD). To understand better this concept, here the Cypher query defined for the messages coming from the topic _customers_.

[source,cypher]
----
WITH event CALL {
  WITH event
  WITH event
  WHERE event.op IN ["c", "u", "r"]
  WITH event["after"] AS evData
  MERGE (c:Customer{customerId:evData.customerNumber})
  ON CREATE SET c.name = evData.customerName
  ON MATCH SET c.name = evData.customerName
  UNION
  WITH event
  WITH event
  WHERE event.op IN ["d"]
  WITH event["before"] AS evData
  MATCH (c:Customer{customerId:evData.customerNumber})
  WITH c
  OPTIONAL MATCH (c)-[:PLACED_ORDER]->(o)
  DETACH DELETE c, o }
----

We can see from this query how we handle the different kink of operations, using a UNION clause to work with two different queries: the first one will manage the creation of a Customer node while, the second one, will manage the removal of a Customer node and all the Order nodes connected to it.

== Example of event handling

We can now create some events inside MySql to see if our Cypher query can handle it. Suppose we create a new row in the table _customer_. First, we need to connect to the MySql command line, specifying in the command the credentials of the user we will use (in this case we will use the _debezium_ user) and the database we want to connect (in our case _demo_):

[source,bash]
----
docker exec -it my-sql mysql -u debezium -ppassword -A demo
----

After running the command, we can now create a new row inside the table _customers_.

[source,sql]
----
INSERT INTO `customers`(`customerNumber`,`customerName`,`contactLastName`,`contactFirstName`,`phone`,`addressLine1`,`addressLine2`,`city`,`state`,`postalCode`,`country`,`salesRepEmployeeNumber`,`creditLimit`) VALUE
(500,'foo','bar','uml','40.32.2555','54, rue Royale',NULL,'Nantes',NULL,'44000','France',1370,'21000.00');
----

After the creation, Debezium will create a new event of type _c_ and will submit it to the topic _customers_. We can see the message from the Confluent Control Center instance:

.Message saw from the Confluent Control Center
image::./img/customer_create_message.png[Paper]

When the message is published, the Neo4j Kafka Connect plugin will get the message and will inject the data using the Cypher query predefined for the message's topic. To enter the Neo4j instance in our stack, we can use this command:

[source, bash]
----
docker exec -it neo4j bin/cypher-shell -u neo4j -p connect
----

We can see that a new node with label Customer has been created in our graph, with _customerId_ equal to 500 and with name _foo_:

[source,bash]
----
neo4j@neo4j> MATCH (c:Customer{customerId:500}) RETURN c;
+--------------------------------------------+
| c                                          |
+--------------------------------------------+
| (:Customer {name: "foo", customerId: 500}) |
+--------------------------------------------+
----

Let's create an order for this customer, to see if the query can handle correctly its creation. Like before, we need to add a new row to a table in the _demo_ database, in this case the table _orders_.

[source,sql]
----
INSERT INTO `orders`(`orderNumber`,`orderDate`,`requiredDate`,`shippedDate`,`status`,`comments`,`customerNumber`) VALUE
(20000,'2003-01-06','2003-01-13','2003-01-10','Created',NULL,500);
----

Like earlier, we can see (now from http://localhost:7474[Neo4j Browser^] for clarity) that the query created both the node with label _Order_ and the relationship of type _PLACED_ORDER_ with its customer, using the foreign key contained in the message.

.The customer and its order
image::./img/order_creation.png[Paper]

The order we just created has status _Created_, but suppose that, at some point, the order is shipped, and we want to change its status to _Shipped_. We can update the order from MySql with this query:

[source,sql]
----
UPDATE `orders` SET status='Shipped' WHERE orderNumber=20000;
----

We can see that the node related to the order with _orderNumber=20000_ has now changed status to _Shipped_.

[source,bash]
----
neo4j@neo4j> MATCH (o:Order{orderId:20000}) RETURN o;
+----------------------------------------------+
| o                                            |
+----------------------------------------------+
| (:Order {orderId: 20000, status: "Shipped"}) |
+----------------------------------------------+
----

To show the last possible operation, the deletion of a row, let's delete the customer we created earlier:

[source,sql]
----
DELETE FROM `customers` WHERE customerNumber=500;
----

We can see that now, inside our graph, the customer and its order are not present, because the connector managed the deletion of the nodes related to the deleted rows.

[source, bash]
----
neo4j@neo4j> MATCH (c:Customer{customerId:500}) RETURN c;
+---+
| c |
+---+
+---+

0 rows
ready to start consuming query after 2 ms, results consumed after another 1 ms
neo4j@neo4j> MATCH (o:Order{orderId:20000}) RETURN o;
+---+
| o |
+---+
+---+

0 rows
ready to start consuming query after 2 ms, results consumed after another 1 ms
----


