== Change Data Capture With Kafka From MySql to Neo4j

This demo will show you how you can create a Change Data Capture (CDC) architecture with Kafka, to migrate the data and its changes from MySql (or a different relational database supported from Debezium) to Neo4j. The stack for this demo is as follows:

. A MySql instance, that will be the start of our event stream;
. Our first Kafka Connect instance with the Debezium Plugin, that will read the binlog of the database and stream the events that occur in the database;
. A Kafka instance, to manage the messages created from Debezium;
. Our second Kafka Connect instance with the Neo4j Plugin, that will stream the messages from Kafka to the Neo4j instance, running, for each specified topic, a certain query to manage the possible types of operation that can occur in the MySql database: _c_ (create), _r_ (read), _u_ (update), _d_ (delete);
. A Neo4j instance, that will be the end point of our event stream.

// TODO: understand how we can remove them (the schemaRegistry can be removed by changing the type of format for the message from avro to (String, Json), for the zookeeper i think it's mandatory if you use the official image)
We will also need, to run Kafka and use the Avro message format, the following instances:

. A Zookeeper instance
. A SchemaRegistry instance

Here an image of the stack:

// TODO: Create architecture diagram of the stack with draw.io

To start the demo, run from the directory of the project the following command:
[source,shell]
----
docker-compose up -d
----
The command will use the _docker-compose.yaml_ file present in the project to create the stack that we will use in the demo, pulling the necessary images and building each Docker container.

After starting the stack, the MySql instance will launch the script _script/init-db.sql_, that will create the user _debezium_, with all the privileges necessary to access the binlog, and will create and the database _demo_ with two tables, _customers_ and _orders_.

After around a minute, the stack will be completely up and running, we can check its status:

[source,shell]
----
docker ps
----

You can see that we will have seven different containers, one for each service we defined in our _docker-compose_ file. After around a minute, the Kafka Connect services will have found their broker (the Kafka instance) and will be ready to receive in input the necessary configurations to:

. Define the stream from MySql to Kafka, defining the configuration for Debezium Plugin
. Define the stream from Kafka to Neo4j, definif the configuration for the Kafka Connect Neo4j Plugin.


== Configuring the Debezium Plugin

Now let's take a loot at the configuration for the Debezium connector for Mysql. This connector will read the MySql binlog and create, for each CRUD operation, an event that will be streamed to Kafka.
The configuration is a Json file, containing different fields that will define how Debezium will create its messages.

[source,json]
----
include::kakfaConnectConfigs/debeziumConfig.source.avro.mysql.json[]
----

Just to understand the core of the configuration, note that:

. You must include in the Debezium config username and password to give the connector access to the database with its user, in this case called _debezium_.

. The _key/value.converter_ fields are mandatory to define what type of messages will be streamed from Debezium. In this case we used the Avro format for the messages, including in the configuration the address of the SchemaRegistry service that will keep the schema to decode the messages.

. The _table.whitelist_ field is used to stream events only from the tables we desire, in this case the tables _customers_ and _orders_ inside the database demo.

. The _transform_ fields are used to change the format of the message or to define, like in our case, the topic where the messages will be subscribed.

In this demo, we will have two topics, called _customers_ and _orders_, created by keeping only the name of the MySql table from the topic created from debezium, that is:

----
database.server.name (mysql_connector) + table.whitelist (demo.customers,ecc...)
----

So, let's suppose that the message is coming from the table _demo.customers_, the transformation will be:

----
mysql_connector.demo.customers -> customers
----

This transformation is not mandatory, depends on what you prefer to have as topic name. There are different possible transformations, for further information look https://debezium.io/documentation/reference/stable/transformations/index.html[here^].

Running the script _configDebeziumConnector.sh_ will create a new connector inside the Debezium instance, that will start to stream messages from the MySql instance as soon as it starts. Let's take a look at the messages created from Debezium. To do this, we need a service that will consume messages from a predefined topic, in this case the _schema-registry_ instance. The following command will consume the first ten messages that are inside the topic _customers_

[source, bash]
----
docker exec -it schema-registry kafka-avro-console-consumer --bootstrap-server broker:9092 --topic customers --from-beginning --max-messages 10
----

Here an example of message:

[source, json]
----
{
  "before": null,
  "after": {
    "mysql_connector.demo.customers.Value": {
      "customerNumber": 103,
      "customerName": "Atelier graphique",
      "contactLastName": "Schmitt",
      "contactFirstName": "Carine ",
      "phone": "40.32.2555",
      "addressLine1": "54, rue Royale",
      "addressLine2": null,
      "city": "Nantes",
      "state": null,
      "postalCode": {
        "string": "44000"
      },
      "country": "France",
      "salesRepEmployeeNumber": {
        "int": 1370
      },
      "creditLimit": {
        "double": 21000.0
      }
    }
  },
  "source": {
    "version": "1.9.3.Final",
    "connector": "mysql",
    "name": "mysql_connector",
    "ts_ms": 1656420989704,
    "snapshot": {
      "string": "true"
    },
    "db": "demo",
    "sequence": null,
    "table": {
      "string": "customers"
    },
    "server_id": 0,
    "gtid": null,
    "file": "binlog.000002",
    "pos": 32307,
    "row": 0,
    "thread": null,
    "query": null
  },
  "op": "r",
  "ts_ms": {
    "long": 1656420989707
  },
  "transaction": null
}
----

What is useful for us in this message is most of the time defined inside the fields _before_,_after_ and _op_. This last field is really important because is what tells us the type of event that happened in the database (in this case, _op_ is equal to _r_, meaning that the operation is of type read (snapshot of the database)).
Because the operation is of type _r_, there is nothing inside the _before_ field(a row can't exist before its creation) the data is present only in the _after_ field.

//TODO: too add other types of messages or, at least, explain the difference in terms of format

== Configuring the Kafka Connect Neo4j Plugin

To stream the messages from Kafka to Neo4j, we will use the Kafka Connect instance that has installed the Neo4j Plugin. Like the Debezium Plugin, we need to define a connector that will handle the messages coming from the different topics we have created.
Let's have a look at the configuration:

[source,json]
----
include::kakfaConnectConfigs/configSimpleCypher.sink.avro.neo4j.json[]
----

This configuration and the Debezium's one share some fields (like the _key/value.converter_), but the main difference between them is the field _neo4j.topic.cypher.<topic_name>_. In this field we can define how the connector has to manage the messages from each topic he's subscribed, giving us the possibility to define, for each topic, different queries.

Keep in mind that, because we are working with messages that come from a database that is not another Neo4j instance, we need to manage the CDC directly with the Cypher queries, because otherwise the Neo4j Plugin could not understand how to manage the events coming from Debezium. Also, the Cypher querying is one of the different sink ingestion strategies present in the Neo4j plugin, for further information take a look https://neo4j.com/labs/kafka/4.0/kafka-connect/#kafka-connect-sink-strategies[here^].

Because we are handling the sink ingestion using the Cypher strategy, we need to manage, for each topic, each type of operation (CRUD). To understand better this concept, here the Cypher query to manage the messages coming from the topic _customers_.

[source,cypher]
----
WITH event CALL {
  WITH event
  WITH event
  WHERE event.op IN ["c", "u", "r"]
  WITH event["after"] AS evData
  MERGE (c:Customer{customerId:evData.customerNumber
})
  ON CREATE  SET c.name = evData.customerName ON
MATCH SET c.name = evData.customerName
UNION
WITH event
WITH event
WHERE event.op IN ["d"]
WITH event["before"] AS evData
MATCH (c:Customer{customerId:evData.customerNumber})
WITH c
OPTIONAL MATCH (c)-[:PLACED_ORDER]->(o)
DETACH DELETE c, o }
----