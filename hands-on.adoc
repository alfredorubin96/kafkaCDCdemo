== Change Data Capture With Kafka From MySql to Neo4j

This demo will show you how you can create a Change Data Capture (CDC) architecture with Kafka, to migrate the data and its changes from MySql (or a different relational database supported from Debezium) to Neo4j. The stack for this demo is as follows:

. A MySql instance, that will be the start of our event stream;
. Our first Kafka Connect instance with the Debezium Plugin, that will read the binlog of the database and stream the events that occur in the database;
. A Kafka instance, to manage the messages created from Debezium;
. Our second Kafka Connect instance with the Neo4j Plugin, that will stream the messages from Kafka to the Neo4j instance, running, for each specified topic, a certain query to manage the possible types of operation that can occur in the MySql database: _c_ (create), _r_ (read), _u_ (update), _d_ (delete);
. A Neo4j instance, that will be the end point of our event stream.

// TODO: understand how we can remove them (the schemaRegistry can be removed by changing the type of format for the message from avro to (String, Json), for the zookeeper i think it's mandatory if you use the official image)
We will also need, to run Kafka and use the Avro message format, the following instances:

. A Zookeeper instance
. A SchemaRegistry instance

Here an image of the stack:

// TODO: Create architecture diagram of the stack with draw.io

To start the demo, run from the directory of the project the following command:
[source,shell]
----
docker-compose up -d
----
The command will use the _docker-compose.yaml_ file present in the project to create the stack that we will use in the demo, pulling the necessary images and building each Docker container.

After starting the stack, the MySql instance will launch the script _script/init-db.sql_, that will create the user _debezium_, with all the privileges necessary to access the binlog, and will create and the database _demo_ with two tables, _customers_ and _orders_.

After around a minute, the stack will be completely up and running, we can check its status:

[source,shell]
----
docker ps
----

You can see that we will have seven different containers, one for each service we defined in our _docker-compose_ file. After around a minute, the Kafka Connect services will have found their broker (the Kafka instance) and will be ready to receive in input the necessary configurations to:

. Define the stream from MySql to Kafka, defining the configuration for Debezium Plugin
. Define the stream from Kafka to Neo4j, definif the configuration for the Kafka Connect Neo4j Plugin.


== Configuration For Debezium Plugin

Now let's take a loot at the configuration for the Debezium connector for Mysql. This connector will read the MySql binlog and create, for each CRUD operation, an event that will be streamed to Kafka.
To add the